{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yKE41B_4ckIe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "import src.support_functions as sf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data = pd.read_csv('../data/CHURN_FEATURES.csv')\n",
        "\n",
        "data = data.drop(columns=[\n",
        "    'ID_VENDEDOR',\n",
        "    'ID_CAMPANA',\n",
        "    'ID_VENDEDOR.1',\n",
        "    'ID_CAMPANA.1',\n",
        "    'ID_VENDEDOR.2',\n",
        "    'ID_CAMPANA.2',\n",
        "    'ID_UBICACION',\n",
        "    'ID_VENDEDOR.3',\n",
        "    'ID_CAMPANA.3',\n",
        "    'ID_VENDEDOR.4',\n",
        "    'ID_CAMPANA.4',\n",
        "    'ID_VENDEDOR.5',\n",
        "    'ID_CAMPANA.5',\n",
        "    'NUMERO_CAMPANA',\n",
        "    'NUMERO_CAMPANA.1',\n",
        "    'PROVINCIA',\n",
        "    'DISTRITO',\n",
        "    'ANIO'\n",
        "])\n",
        "\n",
        "data.drop(columns=['EDAD_VENDEDORA'], inplace=True, axis=1)\n",
        "data['ANTIGUEDAD_MESES'] = data['ANTIGUEDAD_MESES'].clip(lower=0)\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz8pESiD5Kz4",
        "outputId": "b34be493-54d7-49b7-d3cb-6ea80682d570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß PREPARACI√ìN DE DATOS:\n",
            "   ‚Ä¢ Tama√±o conjunto entrenamiento: (13648, 46)\n",
            "   ‚Ä¢ Tama√±o conjunto prueba: (3412, 46)\n",
            "   ‚Ä¢ Caracter√≠sticas escaladas ‚úÖ\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nüîß PREPARACI√ìN DE DATOS:\")\n",
        "\n",
        "# Generamos dummy variables\n",
        "data = pd.get_dummies(data, columns=['DEPARTAMENTO'], drop_first=False)\n",
        "data['SEXO'] = data['SEXO'].map({'F': 0, 'M': 1})\n",
        "data['SEXO'] = data['SEXO'].fillna(0)\n",
        "data['TIPO_VENDEDOR'] = data['TIPO_VENDEDOR'].map({'Asesora': 0, 'L√≠der': 1})\n",
        "\n",
        "# Separar caracter√≠sticas y variable objetivo\n",
        "X = data.drop('TARGET_CHURN', axis=1)\n",
        "y = data['TARGET_CHURN']\n",
        "\n",
        "# Divisi√≥n en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "print(f\"   ‚Ä¢ Tama√±o conjunto entrenamiento: {X_train.shape}\")\n",
        "print(f\"   ‚Ä¢ Tama√±o conjunto prueba: {X_test.shape}\")\n",
        "\n",
        "# Escalado de caracter√≠sticas\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"   ‚Ä¢ Caracter√≠sticas escaladas ‚úÖ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmGpS-eB-e0d",
        "outputId": "abe8eaf4-a0bf-4283-bc1a-77d88b3a9cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚öñÔ∏è  BALANCEADO DE DATOS:\n",
            "   ‚Ä¢ Ratio de balance: 0.812\n",
            "   ‚Ä¢ No es necesario balancear los datos\n"
          ]
        }
      ],
      "source": [
        "# =================================\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è  BALANCEADO DE DATOS:\")\n",
        "\n",
        "# Verificar si es necesario balancear\n",
        "class_counts = y_train.value_counts()\n",
        "balance_ratio = min(class_counts) / max(class_counts)\n",
        "print(f\"   ‚Ä¢ Ratio de balance: {balance_ratio:.3f}\")\n",
        "\n",
        "if balance_ratio < 0.8:  # Si el desbalance es significativo\n",
        "    print(f\"   ‚Ä¢ Aplicando SMOTE + Tomek Links...\")\n",
        "\n",
        "    # Aplicar SMOTE + Tomek Links\n",
        "    smote_tomek = SMOTETomek(random_state=42)\n",
        "    X_train_balanced, y_train_balanced = smote_tomek.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "    print(f\"   ‚Ä¢ Datos balanceados:\")\n",
        "    print(f\"     - Antes: {y_train.value_counts().to_dict()}\")\n",
        "    print(f\"     - Despu√©s: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n",
        "\n",
        "    # Visualizar el balanceado\n",
        "    sf.plot_class_balance(pd.Series(y_train_balanced), \"Distribuci√≥n Despu√©s del Balanceado\")\n",
        "\n",
        "    # Usar datos balanceados\n",
        "    X_train_final = X_train_balanced\n",
        "    y_train_final = y_train_balanced\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ No es necesario balancear los datos\")\n",
        "    X_train_final = X_train_scaled\n",
        "    y_train_final = y_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqzllPlY6rPB"
      },
      "source": [
        "#ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfu6WvAH4WHz",
        "outputId": "fea1071a-d9ce-4dbd-e6dc-65b461e13451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚öñÔ∏è  OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS:\n",
            "\n",
            "üîç Modelo: SVM\n",
            "   Par√°metros: {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1, 0.01]}\n",
            "‚úÖ Mejor combinaci√≥n: {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "üìà Mejor F1-Score (cv): 0.7491\n",
            "\n",
            "üîç Modelo: RF\n",
            "   Par√°metros: {'n_estimators': [100, 200], 'max_depth': [5, 10, None], 'min_samples_split': [2, 5, 10]}\n",
            "‚úÖ Mejor combinaci√≥n: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "üìà Mejor F1-Score (cv): 0.7393\n",
            "\n",
            "üîç Modelo: XGB\n",
            "   Par√°metros: {'n_estimators': [100, 200], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2], 'subsample': [0.8, 1.0], 'colsample_bytree': [0.8, 1.0]}\n",
            "‚úÖ Mejor combinaci√≥n: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
            "üìà Mejor F1-Score (cv): 0.7480\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è  OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS:\")\n",
        "\n",
        "# Hiperpar√°metros por modelo\n",
        "params = {\n",
        "    \"SVM\": {\n",
        "        \"kernel\": [\"rbf\"],\n",
        "        \"C\": [0.1, 1, 10],\n",
        "        \"gamma\": [\"scale\", \"auto\", 0.1, 0.01]\n",
        "    },\n",
        "    \"RF\": {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"max_depth\": [5, 10, None],\n",
        "        \"min_samples_split\": [2, 5, 10]\n",
        "    },\n",
        "    \"XGB\": {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"max_depth\": [3, 5, 7],\n",
        "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "        \"subsample\": [0.8, 1.0],\n",
        "        \"colsample_bytree\": [0.8, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Modelos base\n",
        "modelos_gridsearch = {\n",
        "    \"SVM\": SVC(random_state=42, probability=True),\n",
        "    \"RF\": RandomForestClassifier(random_state=42),\n",
        "    \"XGB\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Para guardar resultados\n",
        "models_results = {}\n",
        "\n",
        "# GridSearchCV\n",
        "for name, model in modelos_gridsearch.items():\n",
        "    print(f\"\\nüîç Modelo: {name}\")\n",
        "    print(f\"   Par√°metros: {params[name]}\")\n",
        "    gs = GridSearchCV(model, params[name], cv=5, n_jobs=-1, scoring='f1')\n",
        "    gs.fit(X_train_final, y_train_final)\n",
        "\n",
        "    best_model = gs.best_estimator_\n",
        "    best_params = gs.best_params_\n",
        "    preds = best_model.predict(X_test_scaled)\n",
        "\n",
        "    models_results[name] = {\n",
        "        \"model\": best_model,\n",
        "        \"params\": best_params,\n",
        "        \"predictions\": preds\n",
        "    }\n",
        "\n",
        "    print(f\"‚úÖ Mejor combinaci√≥n: {best_params}\")\n",
        "    print(f\"üìà Mejor F1-Score (cv): {gs.best_score_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snHlRcS0As2D",
        "outputId": "6fed7de7-5c94-4a5b-f4b4-4de0235c9e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä EVALUACI√ìN DE MODELOS:\n",
            "\n",
            "üî∏ SVM:\n",
            "   ‚Ä¢ Accuracy:  0.6761\n",
            "   ‚Ä¢ Precision: 0.6515\n",
            "   ‚Ä¢ Recall:    0.8797\n",
            "   ‚Ä¢ F1-Score:  0.7486\n",
            "   ‚Ä¢ ROC-AUC:   0.7472\n",
            "\n",
            "üìÑ Classification Report (SVM):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7463    0.4293    0.5451      1542\n",
            "           1     0.6515    0.8797    0.7486      1870\n",
            "\n",
            "    accuracy                         0.6761      3412\n",
            "   macro avg     0.6989    0.6545    0.6468      3412\n",
            "weighted avg     0.6944    0.6761    0.6566      3412\n",
            "\n",
            "\n",
            "üî∏ RF:\n",
            "   ‚Ä¢ Accuracy:  0.6782\n",
            "   ‚Ä¢ Precision: 0.6731\n",
            "   ‚Ä¢ Recall:    0.8027\n",
            "   ‚Ä¢ F1-Score:  0.7322\n",
            "   ‚Ä¢ ROC-AUC:   0.7504\n",
            "\n",
            "üìÑ Classification Report (RF):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6878    0.5272    0.5969      1542\n",
            "           1     0.6731    0.8027    0.7322      1870\n",
            "\n",
            "    accuracy                         0.6782      3412\n",
            "   macro avg     0.6805    0.6650    0.6646      3412\n",
            "weighted avg     0.6797    0.6782    0.6711      3412\n",
            "\n",
            "\n",
            "üî∏ XGB:\n",
            "   ‚Ä¢ Accuracy:  0.6744\n",
            "   ‚Ä¢ Precision: 0.6581\n",
            "   ‚Ä¢ Recall:    0.8449\n",
            "   ‚Ä¢ F1-Score:  0.7399\n",
            "   ‚Ä¢ ROC-AUC:   0.7495\n",
            "\n",
            "üìÑ Classification Report (XGB):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7132    0.4676    0.5648      1542\n",
            "           1     0.6581    0.8449    0.7399      1870\n",
            "\n",
            "    accuracy                         0.6744      3412\n",
            "   macro avg     0.6856    0.6562    0.6523      3412\n",
            "weighted avg     0.6830    0.6744    0.6608      3412\n",
            "\n",
            "\n",
            "üìã TABLA COMPARATIVA DE RESULTADOS:\n",
            "============================================================\n",
            "       Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
            "Model                                                \n",
            "SVM      0.6761     0.6515  0.8797    0.7486   0.7472\n",
            "RF       0.6782     0.6731  0.8027    0.7322   0.7504\n",
            "XGB      0.6744     0.6581  0.8449    0.7399   0.7495\n",
            "\n",
            "üèÜ MEJORES MODELOS POR M√âTRICA:\n",
            "   ‚Ä¢ Accuracy  : RF (0.6782)\n",
            "   ‚Ä¢ Precision : RF (0.6731)\n",
            "   ‚Ä¢ Recall    : SVM (0.8797)\n",
            "   ‚Ä¢ F1-Score  : SVM (0.7486)\n",
            "   ‚Ä¢ ROC-AUC   : RF (0.7504)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =================================\n",
        "# üìä EVALUACI√ìN DE MODELOS\n",
        "# =================================\n",
        "\n",
        "print(f\"\\nüìä EVALUACI√ìN DE MODELOS:\")\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, preds, name):\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, preds),\n",
        "        \"Precision\": precision_score(y_test, preds),\n",
        "        \"Recall\": recall_score(y_test, preds),\n",
        "        \"F1-Score\": f1_score(y_test, preds),\n",
        "        \"ROC-AUC\": roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]) if hasattr(model, \"predict_proba\") else roc_auc_score(y_test, model.decision_function(X_test))\n",
        "    }\n",
        "\n",
        "for model_name, results in models_results.items():\n",
        "    model = results['model']\n",
        "    preds = results['predictions']\n",
        "\n",
        "    # Guardar m√©tricas generales\n",
        "    metrics = evaluate_model(model, X_test_scaled, y_test, preds, model_name)\n",
        "    evaluation_results.append(metrics)\n",
        "\n",
        "    # === Imprimir m√©tricas generales ===\n",
        "    print(f\"\\nüî∏ {model_name}:\")\n",
        "    print(f\"   ‚Ä¢ Accuracy:  {metrics['Accuracy']:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Precision: {metrics['Precision']:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Recall:    {metrics['Recall']:.4f}\")\n",
        "    print(f\"   ‚Ä¢ F1-Score:  {metrics['F1-Score']:.4f}\")\n",
        "    print(f\"   ‚Ä¢ ROC-AUC:   {metrics['ROC-AUC']:.4f}\")\n",
        "\n",
        "    # === Reporte detallado por clase ===\n",
        "    print(f\"\\nüìÑ Classification Report ({model_name}):\")\n",
        "    print(classification_report(y_test, preds, digits=4))\n",
        "\n",
        "\n",
        "# === Tabla resumen ===\n",
        "results_df = pd.DataFrame(evaluation_results).set_index('Model').round(4)\n",
        "\n",
        "print(f\"\\nüìã TABLA COMPARATIVA DE RESULTADOS:\")\n",
        "print(\"=\" * 60)\n",
        "print(results_df)\n",
        "\n",
        "# === Mejores modelos por m√©trica ===\n",
        "print(f\"\\nüèÜ MEJORES MODELOS POR M√âTRICA:\")\n",
        "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']:\n",
        "    best_model = results_df[metric].idxmax()\n",
        "    best_score = results_df[metric].max()\n",
        "    print(f\"   ‚Ä¢ {metric:<10}: {best_model} ({best_score:.4f})\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MITxPRO",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
